{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75afc8f6-68de-4646-a9f9-f1b6df220a40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T02:00:28.749643Z",
     "iopub.status.busy": "2022-08-24T02:00:28.748880Z",
     "iopub.status.idle": "2022-08-24T02:00:29.511265Z",
     "shell.execute_reply": "2022-08-24T02:00:29.510594Z",
     "shell.execute_reply.started": "2022-08-24T02:00:28.749603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 24 02:00:29 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  RTX A6000           Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   34C    P8    26W / 300W |      0MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544f8659-e453-4905-9e93-072ee55b10c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T02:00:36.079218Z",
     "iopub.status.busy": "2022-08-24T02:00:36.078643Z",
     "iopub.status.idle": "2022-08-24T02:00:38.681183Z",
     "shell.execute_reply": "2022-08-24T02:00:38.680387Z",
     "shell.execute_reply.started": "2022-08-24T02:00:36.079192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.8/site-packages (0.13.2)\n",
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.8/site-packages (0.12.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (0.1.97)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.8/site-packages (0.2.2)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from wandb) (59.5.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.9.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.8/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from seqeval) (1.22.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.8/site-packages (from seqeval) (0.24.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.9.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.6.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb seqeval datasets tokenizers sentencepiece evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e4329ed-4547-428f-a45b-3662a4369d06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T02:00:48.975959Z",
     "iopub.status.busy": "2022-08-24T02:00:48.975272Z",
     "iopub.status.idle": "2022-08-24T02:01:02.310237Z",
     "shell.execute_reply": "2022-08-24T02:01:02.309470Z",
     "shell.execute_reply.started": "2022-08-24T02:00:48.975919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-unjg81cl\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-unjg81cl\n",
      "  Resolved https://github.com/huggingface/transformers to commit 6faf283288ce3390281ad8c1d37ccb13f2d03990\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (3.4.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (2022.1.18)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.22.0.dev0) (1.22.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.8.1->transformers==4.22.0.dev0) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.22.0.dev0) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.22.0.dev0) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.22.0.dev0) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.22.0.dev0) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.22.0.dev0) (2021.10.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbfc8e26-8d8e-4884-802e-7cffa275f3e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T02:01:09.929831Z",
     "iopub.status.busy": "2022-08-24T02:01:09.929548Z",
     "iopub.status.idle": "2022-08-24T02:01:12.386028Z",
     "shell.execute_reply": "2022-08-24T02:01:12.385551Z",
     "shell.execute_reply.started": "2022-08-24T02:01:09.929801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (2.9.2-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n",
      "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91acce63-394a-4acf-9dce-c2d0621d150e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T02:01:18.037152Z",
     "iopub.status.busy": "2022-08-24T02:01:18.036318Z",
     "iopub.status.idle": "2022-08-24T02:01:18.044147Z",
     "shell.execute_reply": "2022-08-24T02:01:18.043312Z",
     "shell.execute_reply.started": "2022-08-24T02:01:18.037103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=Roman-Numeral-Classification-Using-run_img_cls.py-on-HuggingFace\n",
      "env: RUN_IMG_CLS_PATH=/storage/transformers/examples/pytorch/image-classification/run_image_classification.py\n"
     ]
    }
   ],
   "source": [
    "HUGGINGFACE_MODEL_NAME = 'Roman-Numeral-Classification'\n",
    "HUGGINGFACE_MODEL_ID   = 'farleyknight/' + HUGGINGFACE_MODEL_NAME\n",
    "\n",
    "%env WANDB_PROJECT={HUGGINGFACE_MODEL_NAME}-Using-run_img_cls.py-on-HuggingFace\n",
    "%env RUN_IMG_CLS_PATH = /storage/transformers/examples/pytorch/image-classification/run_image_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "577c9654-c39c-494f-9828-364b85e231ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T02:01:47.815394Z",
     "iopub.status.busy": "2022-08-24T02:01:47.814778Z",
     "iopub.status.idle": "2022-08-24T02:01:48.024495Z",
     "shell.execute_reply": "2022-08-24T02:01:48.024102Z",
     "shell.execute_reply.started": "2022-08-24T02:01:47.815371Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfarleyknight\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /root/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "WANDB_TOKEN = '760d6f682b2978cc4db05e6ec7e5ced94972a47c'\n",
    "HUGGINGFACE_TOKEN = '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'\n",
    "\n",
    "import wandb\n",
    "from huggingface_hub.commands.user import _login\n",
    "from huggingface_hub.hf_api import HfApi\n",
    "\n",
    "def do_login():\n",
    "    # Weights & Biases tracks our runs\n",
    "    wandb.login(key=WANDB_TOKEN)\n",
    "    # HuggingFace\n",
    "    _login(HfApi(), token=HUGGINGFACE_TOKEN)\n",
    "    \n",
    "do_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb39db9-a4c0-4b02-bf7b-0e2530add2ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-24T02:13:03.341719Z",
     "iopub.status.busy": "2022-08-24T02:13:03.340983Z",
     "iopub.status.idle": "2022-08-24T02:23:13.119773Z",
     "shell.execute_reply": "2022-08-24T02:23:13.118968Z",
     "shell.execute_reply.started": "2022-08-24T02:13:03.341687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/training_args.py:1235: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case farleyknight/vit-base-roman-numeral).\n",
      "  warnings.warn(\n",
      "08/24/2022 02:13:05 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "08/24/2022 02:13:05 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=farleyknight/vit-base-roman-numeral,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/storage/img-cls-data/roman_numeral_outputs/runs/Aug24_02-13-05_psiguc7f9,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/storage/img-cls-data/roman_numeral_outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=True,\n",
      "push_to_hub_model_id=vit-base-roman-numeral,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/storage/img-cls-data/roman_numeral_outputs/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "08/24/2022 02:13:06 - WARNING - datasets.builder - Using custom data configuration farleyknight--roman_numerals-bc0b5430f5222674\n",
      "08/24/2022 02:13:06 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/farleyknight___parquet/farleyknight--roman_numerals-bc0b5430f5222674/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 618.36it/s]\n",
      "08/24/2022 02:13:06 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/farleyknight___parquet/farleyknight--roman_numerals-bc0b5430f5222674/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9379d8cb878e09ce.arrow\n",
      "08/24/2022 02:13:06 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /root/.cache/huggingface/datasets/farleyknight___parquet/farleyknight--roman_numerals-bc0b5430f5222674/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5e4501ce84a2a599.arrow and /root/.cache/huggingface/datasets/farleyknight___parquet/farleyknight--roman_numerals-bc0b5430f5222674/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-c338933470595941.arrow\n",
      "[INFO|configuration_utils.py:643] 2022-08-24 02:13:07,127 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/1ba429d32753f33a0660b80ac6f43a3c80c18938/config.json\n",
      "[INFO|configuration_utils.py:695] 2022-08-24 02:13:07,128 >> Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"i\",\n",
      "    \"1\": \"ii\",\n",
      "    \"2\": \"iii\",\n",
      "    \"3\": \"iv\",\n",
      "    \"4\": \"ix\",\n",
      "    \"5\": \"v\",\n",
      "    \"6\": \"vi\",\n",
      "    \"7\": \"vii\",\n",
      "    \"8\": \"viii\",\n",
      "    \"9\": \"x\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"i\": \"0\",\n",
      "    \"ii\": \"1\",\n",
      "    \"iii\": \"2\",\n",
      "    \"iv\": \"3\",\n",
      "    \"ix\": \"4\",\n",
      "    \"v\": \"5\",\n",
      "    \"vi\": \"6\",\n",
      "    \"vii\": \"7\",\n",
      "    \"viii\": \"8\",\n",
      "    \"x\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.22.0.dev0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2067] 2022-08-24 02:13:07,204 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/1ba429d32753f33a0660b80ac6f43a3c80c18938/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2491] 2022-08-24 02:13:07,997 >> Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2503] 2022-08-24 02:13:07,997 >> Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|feature_extraction_utils.py:432] 2022-08-24 02:13:08,049 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/1ba429d32753f33a0660b80ac6f43a3c80c18938/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:643] 2022-08-24 02:13:13,112 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224-in21k/snapshots/1ba429d32753f33a0660b80ac6f43a3c80c18938/config.json\n",
      "[INFO|configuration_utils.py:695] 2022-08-24 02:13:13,112 >> Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.22.0.dev0\"\n",
      "}\n",
      "\n",
      "[INFO|feature_extraction_utils.py:469] 2022-08-24 02:13:13,113 >> Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "Cloning https://huggingface.co/farleyknight/vit-base-roman-numeral into local empty directory.\n",
      "08/24/2022 02:14:29 - WARNING - huggingface_hub.repository - Cloning https://huggingface.co/farleyknight/vit-base-roman-numeral into local empty directory.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1612] 2022-08-24 02:14:37,446 >> ***** Running training *****\n",
      "[INFO|trainer.py:1613] 2022-08-24 02:14:37,447 >>   Num examples = 2309\n",
      "[INFO|trainer.py:1614] 2022-08-24 02:14:37,447 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1615] 2022-08-24 02:14:37,447 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1616] 2022-08-24 02:14:37,447 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1617] 2022-08-24 02:14:37,447 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1618] 2022-08-24 02:14:37,447 >>   Total optimization steps = 1445\n",
      "[INFO|integrations.py:607] 2022-08-24 02:14:37,525 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfarleyknight\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/storage/mnist-notebooks/wandb/run-20220824_021437-12k8rwa3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/storage/img-cls-data/roman_numeral_outputs/\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/farleyknight/Roman-Numeral-Classification-Using-run_img_cls.py-on-HuggingFace\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/farleyknight/Roman-Numeral-Classification-Using-run_img_cls.py-on-HuggingFace/runs/12k8rwa3\u001b[0m\n",
      "{'loss': 2.2093, 'learning_rate': 1.8615916955017305e-05, 'epoch': 0.35}        \n",
      "{'loss': 1.9053, 'learning_rate': 1.7231833910034604e-05, 'epoch': 0.69}        \n",
      " 20%|███████▉                                | 288/1445 [00:31<02:03,  9.38it/s][INFO|trainer.py:2902] 2022-08-24 02:15:15,668 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2904] 2022-08-24 02:15:15,668 >>   Num examples = 408\n",
      "[INFO|trainer.py:2907] 2022-08-24 02:15:15,669 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▍                                        | 4/51 [00:00<00:01, 38.97it/s]\u001b[A\n",
      " 16%|██████▉                                     | 8/51 [00:00<00:01, 34.13it/s]\u001b[A\n",
      " 24%|██████████                                 | 12/51 [00:00<00:01, 28.73it/s]\u001b[A\n",
      " 29%|████████████▋                              | 15/51 [00:00<00:01, 28.85it/s]\u001b[A\n",
      " 35%|███████████████▏                           | 18/51 [00:00<00:01, 27.86it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 21/51 [00:00<00:01, 27.47it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 24/51 [00:00<00:00, 27.41it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 27/51 [00:00<00:00, 27.37it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 30/51 [00:01<00:00, 26.30it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 33/51 [00:01<00:00, 25.55it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 36/51 [00:01<00:00, 26.35it/s]\u001b[A\n",
      " 76%|████████████████████████████████▉          | 39/51 [00:01<00:00, 26.83it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 42/51 [00:01<00:00, 27.40it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 45/51 [00:01<00:00, 27.59it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 48/51 [00:01<00:00, 26.74it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.324064016342163, 'eval_accuracy': 0.7107843137254902, 'eval_runtime': 1.9147, 'eval_samples_per_second': 213.087, 'eval_steps_per_second': 26.636, 'epoch': 1.0}\n",
      " 20%|████████                                | 289/1445 [00:34<02:03,  9.38it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:02<00:00, 26.93it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2651] 2022-08-24 02:15:18,014 >> Saving model checkpoint to /storage/img-cls-data/roman_numeral_outputs/checkpoint-289\n",
      "[INFO|configuration_utils.py:440] 2022-08-24 02:15:18,019 >> Configuration saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-289/config.json\n",
      "[INFO|modeling_utils.py:1569] 2022-08-24 02:15:20,824 >> Model weights saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-289/pytorch_model.bin\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:15:20,834 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-289/preprocessor_config.json\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:15:25,358 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/preprocessor_config.json\n",
      "{'loss': 1.6347, 'learning_rate': 1.5847750865051904e-05, 'epoch': 1.04}        \n",
      "{'loss': 1.4087, 'learning_rate': 1.4463667820069205e-05, 'epoch': 1.38}        \n",
      "{'loss': 1.3293, 'learning_rate': 1.3079584775086506e-05, 'epoch': 1.73}        \n",
      " 40%|████████████████                        | 578/1445 [01:26<01:24, 10.26it/s][INFO|trainer.py:2902] 2022-08-24 02:16:10,738 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2904] 2022-08-24 02:16:10,739 >>   Num examples = 408\n",
      "[INFO|trainer.py:2907] 2022-08-24 02:16:10,739 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▍                                        | 4/51 [00:00<00:01, 33.74it/s]\u001b[A\n",
      " 16%|██████▉                                     | 8/51 [00:00<00:01, 27.35it/s]\u001b[A\n",
      " 22%|█████████▎                                 | 11/51 [00:00<00:01, 27.13it/s]\u001b[A\n",
      " 27%|███████████▊                               | 14/51 [00:00<00:01, 27.38it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 17/51 [00:00<00:01, 27.26it/s]\u001b[A\n",
      " 39%|████████████████▊                          | 20/51 [00:00<00:01, 27.03it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 23/51 [00:00<00:01, 26.76it/s]\u001b[A\n",
      " 51%|█████████████████████▉                     | 26/51 [00:00<00:01, 24.17it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 29/51 [00:01<00:00, 25.04it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 32/51 [00:01<00:00, 25.23it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 36/51 [00:01<00:00, 27.05it/s]\u001b[A\n",
      " 76%|████████████████████████████████▉          | 39/51 [00:01<00:00, 25.74it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 43/51 [00:01<00:00, 27.24it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▊    | 46/51 [00:01<00:00, 26.90it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.933335542678833, 'eval_accuracy': 0.7892156862745098, 'eval_runtime': 1.9755, 'eval_samples_per_second': 206.526, 'eval_steps_per_second': 25.816, 'epoch': 2.0}\n",
      " 40%|████████████████                        | 578/1445 [01:28<01:24, 10.26it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:01<00:00, 27.26it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2651] 2022-08-24 02:16:12,767 >> Saving model checkpoint to /storage/img-cls-data/roman_numeral_outputs/checkpoint-578\n",
      "[INFO|configuration_utils.py:440] 2022-08-24 02:16:12,772 >> Configuration saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-578/config.json\n",
      "[INFO|modeling_utils.py:1569] 2022-08-24 02:16:15,433 >> Model weights saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-578/pytorch_model.bin\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:16:15,448 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-578/preprocessor_config.json\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:16:21,536 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/preprocessor_config.json\n",
      "{'loss': 1.2134, 'learning_rate': 1.1695501730103806e-05, 'epoch': 2.08}        \n",
      "{'loss': 1.1315, 'learning_rate': 1.0311418685121109e-05, 'epoch': 2.42}        \n",
      "{'loss': 1.1251, 'learning_rate': 8.92733564013841e-06, 'epoch': 2.77}          \n",
      " 60%|████████████████████████                | 867/1445 [02:56<00:51, 11.18it/s][INFO|trainer.py:2902] 2022-08-24 02:17:40,321 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2904] 2022-08-24 02:17:40,321 >>   Num examples = 408\n",
      "[INFO|trainer.py:2907] 2022-08-24 02:17:40,322 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▍                                        | 4/51 [00:00<00:01, 35.42it/s]\u001b[A\n",
      " 16%|██████▉                                     | 8/51 [00:00<00:01, 31.71it/s]\u001b[A\n",
      " 24%|██████████                                 | 12/51 [00:00<00:01, 28.29it/s]\u001b[A\n",
      " 29%|████████████▋                              | 15/51 [00:00<00:01, 27.86it/s]\u001b[A\n",
      " 35%|███████████████▏                           | 18/51 [00:00<00:01, 27.75it/s]\u001b[A\n",
      " 41%|█████████████████▋                         | 21/51 [00:00<00:01, 23.50it/s]\u001b[A\n",
      " 47%|████████████████████▏                      | 24/51 [00:00<00:01, 24.60it/s]\u001b[A\n",
      " 53%|██████████████████████▊                    | 27/51 [00:01<00:00, 25.79it/s]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 30/51 [00:01<00:00, 26.21it/s]\u001b[A\n",
      " 65%|███████████████████████████▊               | 33/51 [00:01<00:00, 27.11it/s]\u001b[A\n",
      " 71%|██████████████████████████████▎            | 36/51 [00:01<00:00, 27.54it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 40/51 [00:01<00:00, 29.02it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▎      | 43/51 [00:01<00:00, 24.82it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▊    | 46/51 [00:01<00:00, 24.60it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.7988855838775635, 'eval_accuracy': 0.7843137254901961, 'eval_runtime': 1.9923, 'eval_samples_per_second': 204.79, 'eval_steps_per_second': 25.599, 'epoch': 3.0}\n",
      " 60%|████████████████████████                | 867/1445 [02:58<00:51, 11.18it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:01<00:00, 25.16it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2651] 2022-08-24 02:17:42,369 >> Saving model checkpoint to /storage/img-cls-data/roman_numeral_outputs/checkpoint-867\n",
      "[INFO|configuration_utils.py:440] 2022-08-24 02:17:42,372 >> Configuration saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-867/config.json\n",
      "[INFO|modeling_utils.py:1569] 2022-08-24 02:17:44,613 >> Model weights saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-867/pytorch_model.bin\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:17:44,615 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-867/preprocessor_config.json\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:17:51,153 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/preprocessor_config.json\n",
      "{'loss': 1.0903, 'learning_rate': 7.5432525951557104e-06, 'epoch': 3.11}        \n",
      "{'loss': 1.05, 'learning_rate': 6.159169550173011e-06, 'epoch': 3.46}           \n",
      "{'loss': 0.9837, 'learning_rate': 4.775086505190312e-06, 'epoch': 3.81}         \n",
      " 80%|███████████████████████████████▏       | 1155/1445 [04:25<00:26, 10.77it/s][INFO|trainer.py:2902] 2022-08-24 02:19:09,087 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2904] 2022-08-24 02:19:09,087 >>   Num examples = 408\n",
      "[INFO|trainer.py:2907] 2022-08-24 02:19:09,088 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▍                                        | 4/51 [00:00<00:01, 38.17it/s]\u001b[A\n",
      " 16%|██████▉                                     | 8/51 [00:00<00:01, 30.92it/s]\u001b[A\n",
      " 24%|██████████                                 | 12/51 [00:00<00:01, 29.51it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 16/51 [00:00<00:01, 25.42it/s]\u001b[A\n",
      " 37%|████████████████                           | 19/51 [00:00<00:01, 25.21it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 22/51 [00:00<00:01, 26.37it/s]\u001b[A\n",
      " 49%|█████████████████████                      | 25/51 [00:00<00:00, 26.50it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 28/51 [00:01<00:00, 23.89it/s]\u001b[A\n",
      " 61%|██████████████████████████▏                | 31/51 [00:01<00:00, 24.38it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 34/51 [00:01<00:00, 25.44it/s]\u001b[A\n",
      " 73%|███████████████████████████████▏           | 37/51 [00:01<00:00, 26.51it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▋         | 40/51 [00:01<00:00, 27.03it/s]\u001b[A\n",
      " 86%|█████████████████████████████████████      | 44/51 [00:01<00:00, 27.04it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 47/51 [00:01<00:00, 27.25it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6956034898757935, 'eval_accuracy': 0.8186274509803921, 'eval_runtime': 1.946, 'eval_samples_per_second': 209.665, 'eval_steps_per_second': 26.208, 'epoch': 4.0}\n",
      " 80%|███████████████████████████████▏       | 1156/1445 [04:27<00:26, 10.77it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:01<00:00, 26.82it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2651] 2022-08-24 02:19:11,067 >> Saving model checkpoint to /storage/img-cls-data/roman_numeral_outputs/checkpoint-1156\n",
      "[INFO|configuration_utils.py:440] 2022-08-24 02:19:11,081 >> Configuration saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-1156/config.json\n",
      "[INFO|modeling_utils.py:1569] 2022-08-24 02:19:13,119 >> Model weights saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-1156/pytorch_model.bin\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:19:13,152 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-1156/preprocessor_config.json\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:19:19,636 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/preprocessor_config.json\n",
      " 80%|███████████████████████████████▏       | 1156/1445 [04:43<00:26, 10.77it/s][INFO|trainer.py:2729] 2022-08-24 02:20:11,247 >> Deleting older checkpoint [/storage/img-cls-data/roman_numeral_outputs/checkpoint-289] due to args.save_total_limit\n",
      "{'loss': 1.0053, 'learning_rate': 3.3910034602076125e-06, 'epoch': 4.15}        \n",
      "{'loss': 0.9802, 'learning_rate': 2.0069204152249138e-06, 'epoch': 4.5}         \n",
      "{'loss': 0.999, 'learning_rate': 6.228373702422146e-07, 'epoch': 4.84}          \n",
      "100%|██████████████████████████████████████▉| 1444/1445 [05:57<00:00,  9.41it/s][INFO|trainer.py:2902] 2022-08-24 02:20:41,786 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2904] 2022-08-24 02:20:41,786 >>   Num examples = 408\n",
      "[INFO|trainer.py:2907] 2022-08-24 02:20:41,786 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/51 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████▎                                       | 5/51 [00:00<00:01, 38.80it/s]\u001b[A\n",
      " 18%|███████▊                                    | 9/51 [00:00<00:01, 33.44it/s]\u001b[A\n",
      " 25%|██████████▉                                | 13/51 [00:00<00:01, 28.05it/s]\u001b[A\n",
      " 31%|█████████████▍                             | 16/51 [00:00<00:01, 28.01it/s]\u001b[A\n",
      " 37%|████████████████                           | 19/51 [00:00<00:01, 26.90it/s]\u001b[A\n",
      " 43%|██████████████████▌                        | 22/51 [00:00<00:01, 25.10it/s]\u001b[A\n",
      " 49%|█████████████████████                      | 25/51 [00:00<00:00, 26.30it/s]\u001b[A\n",
      " 57%|████████████████████████▍                  | 29/51 [00:01<00:00, 27.49it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 32/51 [00:01<00:00, 27.53it/s]\u001b[A\n",
      " 69%|█████████████████████████████▌             | 35/51 [00:01<00:00, 28.11it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 38/51 [00:01<00:00, 28.45it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▌        | 41/51 [00:01<00:00, 28.42it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▉     | 45/51 [00:01<00:00, 28.13it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 48/51 [00:01<00:00, 27.68it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.689139723777771, 'eval_accuracy': 0.8308823529411765, 'eval_runtime': 1.927, 'eval_samples_per_second': 211.724, 'eval_steps_per_second': 26.466, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1445/1445 [05:59<00:00,  9.41it/s]\n",
      "100%|███████████████████████████████████████████| 51/51 [00:01<00:00, 26.60it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:2651] 2022-08-24 02:20:43,740 >> Saving model checkpoint to /storage/img-cls-data/roman_numeral_outputs/checkpoint-1445\n",
      "[INFO|configuration_utils.py:440] 2022-08-24 02:20:43,823 >> Configuration saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-1445/config.json\n",
      "[INFO|modeling_utils.py:1569] 2022-08-24 02:20:46,265 >> Model weights saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-1445/pytorch_model.bin\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:20:46,266 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/checkpoint-1445/preprocessor_config.json\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:20:52,699 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/preprocessor_config.json\n",
      "[INFO|trainer.py:2729] 2022-08-24 02:21:43,188 >> Deleting older checkpoint [/storage/img-cls-data/roman_numeral_outputs/checkpoint-578] due to args.save_total_limit\n",
      "[INFO|trainer.py:1857] 2022-08-24 02:21:43,634 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1951] 2022-08-24 02:21:43,635 >> Loading best model from /storage/img-cls-data/roman_numeral_outputs/checkpoint-1445 (score: 0.689139723777771).\n",
      "{'train_runtime': 426.5695, 'train_samples_per_second': 27.065, 'train_steps_per_second': 3.387, 'train_loss': 1.2808735091793495, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████████| 1445/1445 [07:00<00:00,  3.44it/s]\n",
      "[INFO|trainer.py:2651] 2022-08-24 02:21:44,389 >> Saving model checkpoint to /storage/img-cls-data/roman_numeral_outputs/\n",
      "[INFO|configuration_utils.py:440] 2022-08-24 02:21:44,957 >> Configuration saved in /storage/img-cls-data/roman_numeral_outputs/config.json\n",
      "[INFO|modeling_utils.py:1569] 2022-08-24 02:21:45,812 >> Model weights saved in /storage/img-cls-data/roman_numeral_outputs/pytorch_model.bin\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:21:46,453 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/preprocessor_config.json\n",
      "[INFO|trainer.py:2651] 2022-08-24 02:21:47,046 >> Saving model checkpoint to /storage/img-cls-data/roman_numeral_outputs/\n",
      "[INFO|configuration_utils.py:440] 2022-08-24 02:21:47,742 >> Configuration saved in /storage/img-cls-data/roman_numeral_outputs/config.json\n",
      "[INFO|modeling_utils.py:1569] 2022-08-24 02:21:48,845 >> Model weights saved in /storage/img-cls-data/roman_numeral_outputs/pytorch_model.bin\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:21:49,285 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/preprocessor_config.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "08/24/2022 02:22:06 - WARNING - huggingface_hub.repository - Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n",
      "08/24/2022 02:22:06 - WARNING - huggingface_hub.repository - The progress bars may be unreliable.\n",
      "Upload file runs/Aug24_02-13-05_psiguc7f9/events.out.tfevents.1661307277.psiguc7remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/farleyknight/vit-base-roman-numeral\n",
      "   e440daa..165725b  main -> main\n",
      "\n",
      "08/24/2022 02:22:11 - WARNING - huggingface_hub.repository - remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/farleyknight/vit-base-roman-numeral\n",
      "   e440daa..165725b  main -> main\n",
      "\n",
      "Upload file runs/Aug24_02-13-05_psiguc7f9/events.out.tfevents.1661307277.psiguc7\n",
      "To https://huggingface.co/farleyknight/vit-base-roman-numeral\n",
      "   165725b..beb2424  main -> main\n",
      "\n",
      "08/24/2022 02:22:26 - WARNING - huggingface_hub.repository - To https://huggingface.co/farleyknight/vit-base-roman-numeral\n",
      "   165725b..beb2424  main -> main\n",
      "\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     1.2809\n",
      "  train_runtime            = 0:07:06.56\n",
      "  train_samples_per_second =     27.065\n",
      "  train_steps_per_second   =      3.387\n",
      "[INFO|trainer.py:2902] 2022-08-24 02:22:29,284 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2904] 2022-08-24 02:22:29,284 >>   Num examples = 408\n",
      "[INFO|trainer.py:2907] 2022-08-24 02:22:29,285 >>   Batch size = 8\n",
      "100%|███████████████████████████████████████████| 51/51 [00:01<00:00, 26.22it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =     0.8309\n",
      "  eval_loss               =     0.6891\n",
      "  eval_runtime            = 0:00:02.00\n",
      "  eval_samples_per_second =    203.008\n",
      "  eval_steps_per_second   =     25.376\n",
      "[INFO|trainer.py:2651] 2022-08-24 02:22:32,089 >> Saving model checkpoint to /storage/img-cls-data/roman_numeral_outputs/\n",
      "[INFO|configuration_utils.py:440] 2022-08-24 02:22:32,515 >> Configuration saved in /storage/img-cls-data/roman_numeral_outputs/config.json\n",
      "[INFO|modeling_utils.py:1569] 2022-08-24 02:22:33,304 >> Model weights saved in /storage/img-cls-data/roman_numeral_outputs/pytorch_model.bin\n",
      "[INFO|feature_extraction_utils.py:339] 2022-08-24 02:22:33,859 >> Feature extractor saved in /storage/img-cls-data/roman_numeral_outputs/preprocessor_config.json\n",
      "Upload file runs/Aug24_02-13-05_psiguc7f9/events.out.tfevents.1661307751.psiguc7remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/farleyknight/vit-base-roman-numeral\n",
      "   beb2424..5ef38aa  main -> main\n",
      "\n",
      "08/24/2022 02:22:54 - WARNING - huggingface_hub.repository - remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/farleyknight/vit-base-roman-numeral\n",
      "   beb2424..5ef38aa  main -> main\n",
      "\n",
      "Upload file runs/Aug24_02-13-05_psiguc7f9/events.out.tfevents.1661307751.psiguc7\n",
      "To https://huggingface.co/farleyknight/vit-base-roman-numeral\n",
      "   5ef38aa..35e5df0  main -> main\n",
      "\n",
      "08/24/2022 02:23:06 - WARNING - huggingface_hub.repository - To https://huggingface.co/farleyknight/vit-base-roman-numeral\n",
      "   5ef38aa..35e5df0  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python $RUN_IMG_CLS_PATH \\\n",
    "    --dataset_name farleyknight/roman_numerals \\\n",
    "    --output_dir /storage/img-cls-data/roman_numeral_outputs/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --remove_unused_columns False \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --push_to_hub \\\n",
    "    --push_to_hub_model_id vit-base-roman-numeral \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 100 \\\n",
    "    --evaluation_strategy epoch \\\n",
    "    --save_strategy epoch \\\n",
    "    --load_best_model_at_end True \\\n",
    "    --save_total_limit 3 \\\n",
    "    --seed 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1888ed6e-73d6-407e-85ea-6ac58d5bb413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2df4cd318acb45643199114087e35daed2841a7308bba0392ab72a2eb7bc516b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
